
<h1 align="center"> <p>ğŸ¦š RWKV-PEFT</p></h1>

\[ English | [ä¸­æ–‡](README_zh.md) \]

# Release
- infctx
- fla --fla
- State tuning
- Quant(QPissa,QLora) --quant int8/nf4
- Bone
- Pissa
- Lisa
- Lora
- dataload(getã€padã€only)
### High performance on consumer hardware

Consider the memory requirements for training the following models with an 4090 24GB GPU with 64GB of CPU RAM.(--strategy deepspeed_stage_1 --ctx_len 1024 --micro_bsz 1 --lora_r 64)

|   Model         | Full Finetuning | lora/pissa  | Qlora/Qpissa | State tuning |
| --------- | ---- | ---- | ---- | ---- |
| RWKV6-1.6B | OOM GPU | 7.4GB GPU | 5.6GB GPU | 6.4GB GPU |
| RWKV6-3B | OOM GPU | 12.1GB GPU | 8.2GB GPU | 9.4GB GPU |
| RWKV6-7B | OOM GPU | 23.7GB GPU(bsz 8 OOM) | 14.9GB GPU(bsz 8 need 19.5GB) | 18.1GB GPU |
#### Quant State Tuning
- strategy deepspeed_stage_1
- ctx_len 1024
- micro_bsz 1
- 4090 24G

|   Model         | bf16 | int8  | nf4/fp4/4bit |
| --------- | ---- | ---- | ---- |
| RWKV6-1.6B | 6.1GB GPU | 4.7GB GPU | 4.1GB GPU |
| RWKV6-3B | 9.1GB GPU | 6.5GB GPU | 5.2GB GPU |
| RWKV6-7B | 17.8GB GPU | 11.9GB GPU | 8.5GB GPU |
| RWKV6-14B | xxGB GPU | xxGB GPU | xxGB GPU |
# å¿«é€Ÿå¼€å§‹
æŒ‰ç…§å¿…è¦ä¾èµ–
```
pip install requirements.txt
```
å‚è€ƒscriptsä¸­çš„ç¤ºä¾‹ä¿®æ”¹è·¯å¾„ä»¥åŠæ‰€éœ€å‚æ•°ï¼ˆæ•°æ®å‡†å¤‡è¯¦ç»†å‚è€ƒRWKVå®˜æ–¹æ•™ç¨‹ï¼‰
```
sh scripts/run_lora.sh
```
# å…·ä½“ä½¿ç”¨
- peft  
å‚æ•°peftä¸­åŒ…å«å¤šä¸ªæ–¹æ³•ï¼Œè¯¦ç»†æŸ¥çœ‹ç®€ä»‹ï¼Œé€‰æ‹©æ‰€éœ€çš„æ–¹æ³•åè¦é…ç½®ç›¸åº”çš„config
ä¾‹å¦‚ï¼š
```
--peft bone --bone_config $bone_config
```
- train_parts  
æ›´è‡ªç”±çš„é€‰æ‹©è®­ç»ƒéƒ¨åˆ†ï¼Œå¦‚"emb","head","time","ln".å¦‚æœåªå¯¹k,véƒ¨åˆ†è®­ç»ƒåªéœ€è¦è®¾ç½®[]å³å¯
ä¾‹å¦‚ï¼š
```
--train_parts ["time", "ln"]
```
- Quant  
åœ¨ä½¿ç”¨peftæˆ–state tuningæ—¶å¯ä½¿ç”¨Quanté‡åŒ–æƒé‡ä»¥å‡å°‘æ˜¾å­˜å ç”¨
```
--quant int8/nf4
```
- infctx  
RWKVç³»åˆ—ç‰¹æœ‰çš„è®­ç»ƒæ–¹å¼ï¼ˆæ— é™é•¿åº¦è®­ç»ƒï¼‰ï¼Œå¯é…åˆä»»æ„å¾®è°ƒæ–¹æ³•ä¸€èµ·ä½¿ç”¨ï¼Œé˜²æ­¢è®­ç»ƒæ•°æ®è¿‡é•¿å¯¼è‡´æ˜¾å­˜çˆ†ç‚¸
ctx_lenä¸ºä½ æƒ³è®­ç»ƒçš„é•¿åº¦ï¼ˆæ ¹æ®è®­ç»ƒæ•°æ®é•¿åº¦è®¾ç½®ï¼‰ 
chunk_ctxæ ¹æ®æ˜¾å­˜é€‚å½“è°ƒæ•´ï¼Œchunk_ctxæ˜¯ä»ctx_lenåˆ‡ç‰‡å¾—åˆ°æ‰€ä»¥ä¿è¯chunk_ctxå°äºctx_len
æ·»åŠ è„šæœ¬å‚æ•°å¦‚ä¸‹ï¼š
```
--train_type infctx --chunk_ctx 512 --ctx_len 2048
```
- State tuning  
RWKVç‰¹æœ‰çš„å¾®è°ƒæ–¹æ³•ï¼Œè®­ç»ƒå¼€é”€æä½
- dataload  
æ”¯æŒä¸åŒçš„æ•°æ®é‡‡æ ·ï¼Œé»˜è®¤ä½¿ç”¨get(RWKV-LM)è¿™æ˜¯ä¸€ç§éšæœºé‡‡æ ·ï¼Œå°†æ‰€æœ‰æ•°æ®è§†ä½œä¸€æ¡æ•°æ®æ ¹æ®ctx_lenéšæœºåˆ‡ç‰‡ä¾¿äºå¹¶è¡Œ
padã€onlyéƒ½æ˜¯ä¸ºäº†ä»æ¯æ¡æ•°æ®èµ·å§‹å¼€å§‹é‡‡æ ·
padæœ«å°¾å¡«å……ï¼Œä¾‹å¦‚ctx_lenä¸º1024è€Œå½“å‰é‡‡æ ·æ•°æ®å®é™…é•¿åº¦ä¸º1000åˆ™ä¼šåœ¨æœ«å°¾å¡«å……ä¸‹ä¸€æ¡æ•°æ®å‰24ä¸ªtokenä¾¿äºå¹¶è¡Œ
onlyä»…æ”¯æŒbsz=1çš„æƒ…å†µï¼Œä¸ºäº†ctx_lenè®¾ç½®ä¸ºé‡‡æ ·æœ€å¤§é•¿åº¦ï¼Œå½“å‰é‡‡æ ·æ•°æ®é•¿åº¦å¤§äºctx_lençš„éƒ¨åˆ†ä¼šè¢«æˆªæ–­
```
--dataload pad
```
- loss_mask  
æ”¯æŒå¯¹qa é—®é¢˜éƒ¨åˆ†ä»¥åŠpadæœ«å°¾å¡«å……éƒ¨åˆ†é®æ©ï¼Œé˜²æ­¢æ¨¡å‹æ ¹æ®é¢˜ç›®èƒŒç­”æ¡ˆå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›
```
--loss_mask qa/pad
```
- strategy  
deepspeedæ˜¾å­˜å†…å­˜åˆ†é…ç­–ç•¥,ä¼˜å…ˆä½¿ç”¨1ï¼Œå½“æ¨¡å‹è¾ƒå¤§æˆ–è€…å…¨é‡å¾®è°ƒæ—¶åˆ™ä½¿ç”¨2/3 å¦‚æœä»çˆ†æ˜¾å­˜åˆ™ä½¿ç”¨offloadï¼Œ3å¯ä»¥æ¨¡å‹å¹¶è¡Œï¼ˆä¸€ä¸ªæ¨¡å‹è¢«åˆ‡åˆ†åœ¨å¤šå¡ä¸Šï¼‰
deepspeed_stage_1
deepspeed_stage_2
deepspeed_stage_2_offload
deepspeed_stage_3
deepspeed_stage_3_offload
```
deepspeed_stage_1
```
- ctx_len  
é‡‡æ ·è®­ç»ƒé•¿åº¦ï¼Œæ ¹æ®æ•°æ®é•¿åº¦è¿›è¡Œè°ƒæ•´ï¼Œctx_lenå¢å¤§æ˜¾å­˜ä¹Ÿä¼šéšä¹‹å¢å¤§
- micro_bsz  
